{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470860f-7bb2-4137-88c5-8a7c3440c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bd21b-3cd1-4c42-b2be-2ec809c00433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# init accelerator\n",
    "accelerator = Accelerator(device_placement=True, fp16=True, mixed_precision='fp16')\n",
    "device = accelerator.device\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM_STEPS = 200 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2536c-1ca7-4655-9aac-cb121425b6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.utils import get_teacher_student_tokenizer\n",
    "teacher, student, tokenizer = get_teacher_student_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468a05a-27d7-4067-82f3-1d625ed670ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "import random\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label, length)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    input_labels, output_labels = zip(*data)\n",
    "    lengths = [len(ids) for ids in input_labels]\n",
    "    max_len = max(lengths)\n",
    "    il_padded = []\n",
    "    ol_padded = []\n",
    "    for i in range(len(input_labels)):\n",
    "        il = input_labels[i] + [1] * (max_len - len(input_labels[i]))\n",
    "        ol = output_labels[i] + [1] * (max_len - len(output_labels[i]))\n",
    "        il_padded.append(il)\n",
    "        ol_padded.append(ol)\n",
    "    \n",
    "    il_padded = torch.tensor(il_padded, dtype=torch.long)\n",
    "    ol_padded = torch.tensor(ol_padded, dtype=torch.long)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    \n",
    "    return il_padded, ol_padded, lengths\n",
    "\n",
    "\n",
    "# example dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, sentences_path, sep, tokenizer):\n",
    "        self.sentences = pd.read_csv(sentences_path, sep=sep)['sentence']\n",
    "        self.main_tokenizer = tokenizer\n",
    "        self.aux_tokenizer = MosesTokenizer(lang='pl')\n",
    "        self.vocab = self.main_tokenizer.vocab\n",
    "        self.vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_labels, output_labels = self.random_word(self.sentences[idx])\n",
    "\n",
    "        # [CLS] tag = BOS tag, [SEP] tag = SEP tag\n",
    "        t1 = [self.main_tokenizer.bos_token_id] + input_labels + [self.main_tokenizer.sep_token_id]\n",
    "        t1_label = [self.main_tokenizer.bos_token_id] + output_labels + [self.main_tokenizer.sep_token_id]\n",
    "\n",
    "        bert_input = t1[:self.main_tokenizer.max_len_single_sentence]\n",
    "        bert_label = t1_label[:self.main_tokenizer.max_len_single_sentence]\n",
    "\n",
    "        return bert_input, bert_label\n",
    "    \n",
    "    def random_word(self, sentence):\n",
    "        tokens = self.aux_tokenizer.tokenize(sentence)\n",
    "        input_labels = []\n",
    "        output_labels = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            bpe_tokens = self.main_tokenizer.tokenize(tokens[i])\n",
    "            nb_bpe_tokens = len(bpe_tokens)\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                \n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    mask_id = self.vocab['<mask>']\n",
    "                    input_labels += [mask_id] * nb_bpe_tokens\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for _ in range(nb_bpe_tokens):\n",
    "                        random_id = random.randrange(self.vocab_size)\n",
    "                        input_labels.append(random_id)\n",
    "\n",
    "                # 10% randomly change token to current token\n",
    "                else:\n",
    "                    input_labels += [self.vocab[token] for token in bpe_tokens]\n",
    "\n",
    "                output_labels += [self.vocab[token] for token in bpe_tokens]\n",
    "\n",
    "            else:\n",
    "                input_labels += [self.vocab[token] for token in bpe_tokens]\n",
    "                output_labels += [self.main_tokenizer.pad_token_id] * nb_bpe_tokens\n",
    "        \n",
    "        return input_labels, output_labels\n",
    "\n",
    "train_dataset = CustomImageDataset('datasets/klej_polemo2.0-in/train.tsv', sep='\\t', tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = CustomImageDataset('datasets/klej_polemo2.0-in/dev.tsv', sep='\\t', tokenizer=tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# batch = next(iter(test_loader))\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eef306-c25c-44ee-8405-78cdf18b2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set accelerator\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from trainers.utils import configure_optimizer\n",
    "\n",
    "# wyrzucić z wd embedingi i batchnormalization\n",
    "# optim = configure_optimizer(student, AdamW, weight_decay=1e-3, lr=1e-4)\n",
    "optim = AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "train_loader, test_loader, teacher, student, optim = accelerator.prepare(\n",
    "    train_loader, test_loader, teacher, student, optim)\n",
    "\n",
    "loaders  = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "NUM_TRAINING_STEPS = len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optim,\n",
    "        num_cycles=EPOCHS,\n",
    "        num_warmup_steps=int(0.15 * NUM_TRAINING_STEPS),\n",
    "        num_training_steps=NUM_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb938c-da5a-4722-93d1-1eddc0deb1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trainers.distilTrainer import DistilTrainer\n",
    "\n",
    "params_trainer = {\n",
    "    'teacher': teacher,#.to(device),\n",
    "    'student': student,#.to(device),\n",
    "    'tokenizer': tokenizer,\n",
    "    'loaders': loaders,\n",
    "    'criterion1': nn.CrossEntropyLoss().to(device),\n",
    "    'criterion2': nn.CrossEntropyLoss().to(device),\n",
    "    # 'criterion2': nn.KLDivLoss('batchmean').to(device), # mam używać log_target?\n",
    "    'criterion3': nn.CosineEmbeddingLoss().to(device),\n",
    "    'optim': optim,\n",
    "    'scheduler': scheduler,\n",
    "    'accelerator': accelerator,\n",
    "    'device': device\n",
    "}\n",
    "trainer = DistilTrainer(**params_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27a1cb-3c29-47d4-8692-cc2241ce8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4490a14-def6-4df0-bbee-535fd4f6a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "config_run_epoch = collections.namedtuple('RE', ['save_interval', 'grad_accum_steps', 'running_step'])(100, GRAD_ACCUM_STEPS, 30)\n",
    "\n",
    "params_run = {\n",
    "    'epoch_start': 0,\n",
    "    'epoch_end': EPOCHS,\n",
    "    'exp_name': f'plain_distil_scheduler:cosine,accelerate:bf16,batch_size:{BATCH_SIZE},hwmasking,acc_grad,cos_logits,clip_grad',\n",
    "    'config_run_epoch': config_run_epoch,\n",
    "    'temp': 0.5,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "trainer.run_exp(**params_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249e2fe-f313-41a3-b889-a21f86549fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.n_logger.run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0955caf-e6ad-4315-98e2-d9d92e2750e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e17f9-36ad-4916-bb48-01c4dc509a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "student.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eca18f-4ff9-4d24-89aa-559963ce490e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
